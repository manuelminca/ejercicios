{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercices \n",
    "\n",
    "## Question 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tempfile import NamedTemporaryFile, TemporaryDirectory\n",
    "import json\n",
    "import tarfile\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blob(blob: str) -> TemporaryDirectory:\n",
    "    with NamedTemporaryFile() as f:\n",
    "        temp_dir = TemporaryDirectory()\n",
    "        target_dir = temp_dir.name\n",
    "        with tarfile.open(blob, \"r:gz\") as tf:\n",
    "            tf.extractall(path=target_dir)\n",
    "\n",
    "        return temp_dir\n",
    "\n",
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'blobs/'\n",
    "blobs_files = os.listdir(path)\n",
    "#Creating the folder if it does not exist already\n",
    "target_directory = 'clean_blobs_1/'\n",
    "if not os.path.exists(target_directory):\n",
    "    os.makedirs(target_directory)\n",
    "\n",
    "#iterating through all compressed files\n",
    "for blob_name in blobs_files:\n",
    "    blob_path = os.path.join(path, blob_name)\n",
    "    temp_dir = download_blob(blob_path)\n",
    "    meta_path = os.path.join(temp_dir.name, \"metadata.json\")\n",
    "    result_path = os.path.join(temp_dir.name, \"result.json\")\n",
    "    extracted_data = {}\n",
    "\n",
    "    name = blob_name.split('-')[0]\n",
    "\n",
    "    with open(meta_path, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    with open(result_path, \"r\") as f:\n",
    "        result = json.load(f)\n",
    "\n",
    "    target_file = os.path.join(target_directory, blob_name)\n",
    "    if meta['check_name'] != name:\n",
    "        meta['check_name'] = name\n",
    "\n",
    "        #We need to compress the blob and save it to the target directory\n",
    "        with open('metadata.json', 'w') as outfile:\n",
    "            json.dump(meta, outfile)\n",
    "\n",
    "        with open('result.json', 'w') as outfile:\n",
    "            json.dump(result, outfile)\n",
    "            \n",
    "        with tarfile.open(target_file,\"w:gz\") as tar:\n",
    "            tar.add(os.path.basename('metadata.json'))\n",
    "            tar.add(os.path.basename('result.json'))\n",
    "    else:\n",
    "        copyfile(blob_path, target_file)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "A typical Beam driver program works as follows:\n",
    "\n",
    "1. Create a Pipeline object and set the pipeline execution options, including the Pipeline Runner.\n",
    "2. Create an initial PCollection for pipeline data, either using the IOs to read data from an external storage system, or using a Create transform to build a PCollection from in-memory data.\n",
    "3. Apply PTransforms to each PCollection. Transforms can change, filter, group, analyze, or otherwise process the elements in a PCollection. A transform creates a new output PCollection without modifying the input collection. A typical pipeline applies subsequent transforms to each new output PCollection in turn until processing is complete. However, note that a pipeline does not have to be a single straight line of transforms applied one after another: think of PCollections as variables and PTransforms as functions applied to these variables: the shape of the pipeline can be an arbitrarily complex processing graph.\n",
    "4. Use IOs to write the final, transformed PCollection(s) to an external source.\n",
    "5. Run the pipeline using the designated Pipeline Runner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = PipelineOptions()\n",
    "pipeline = beam.Pipeline(options=options)\n",
    "\n",
    "class MyOptions(PipelineOptions):\n",
    "@classmethod\n",
    "    def _add_argparse_args(cls, parser):\n",
    "        parser.add_argument('--input',\n",
    "                            help='Input for the pipeline',\n",
    "                            default='./blobs/')\n",
    "        parser.add_argument('--output',\n",
    "                            help='Output for the pipeline',\n",
    "                            default='./clean_blobs_2/')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "998e36c689971a6175d8a12518d119b65922ae10d4bf024dd2fd9f854295502d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
