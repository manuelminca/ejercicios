{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercices \n",
    "\n",
    "## Question 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tempfile import NamedTemporaryFile, TemporaryDirectory\n",
    "import json\n",
    "import tarfile\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blob(blob: str) -> TemporaryDirectory:\n",
    "    with NamedTemporaryFile() as f:\n",
    "        temp_dir = TemporaryDirectory()\n",
    "        target_dir = temp_dir.name\n",
    "        with tarfile.open(blob, \"r:gz\") as tf:\n",
    "            tf.extractall(path=target_dir)\n",
    "\n",
    "        return temp_dir\n",
    "\n",
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'blobs/'\n",
    "blobs_files = os.listdir(path)\n",
    "#Creating the folder if it does not exist already\n",
    "target_directory = 'clean_blobs_1/'\n",
    "if not os.path.exists(target_directory):\n",
    "    os.makedirs(target_directory)\n",
    "\n",
    "#iterating through all compressed files\n",
    "for blob_name in blobs_files:\n",
    "    blob_path = os.path.join(path, blob_name)\n",
    "    temp_dir = download_blob(blob_path)\n",
    "    meta_path = os.path.join(temp_dir.name, \"metadata.json\")\n",
    "    result_path = os.path.join(temp_dir.name, \"result.json\")\n",
    "    extracted_data = {}\n",
    "\n",
    "    name = blob_name.split('-')[0]\n",
    "\n",
    "    with open(meta_path, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    with open(result_path, \"r\") as f:\n",
    "        result = json.load(f)\n",
    "\n",
    "    target_file = os.path.join(target_directory, blob_name)\n",
    "    if meta['check_name'] != name:\n",
    "        meta['check_name'] = name\n",
    "\n",
    "        #We need to compress the blob and save it to the target directory\n",
    "        with open('metadata.json', 'w') as outfile:\n",
    "            json.dump(meta, outfile)\n",
    "\n",
    "        with open('result.json', 'w') as outfile:\n",
    "            json.dump(result, outfile)\n",
    "            \n",
    "        with tarfile.open(target_file,\"w:gz\") as tar:\n",
    "            tar.add(os.path.basename('metadata.json'))\n",
    "            tar.add(os.path.basename('result.json'))\n",
    "    else:\n",
    "        copyfile(blob_path, target_file)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "A typical Beam driver program works as follows:\n",
    "\n",
    "1. Create a Pipeline object and set the pipeline execution options, including the Pipeline Runner.\n",
    "2. Create an initial PCollection for pipeline data, either using the IOs to read data from an external storage system, or using a Create transform to build a PCollection from in-memory data.\n",
    "3. Apply PTransforms to each PCollection. Transforms can change, filter, group, analyze, or otherwise process the elements in a PCollection. A transform creates a new output PCollection without modifying the input collection. A typical pipeline applies subsequent transforms to each new output PCollection in turn until processing is complete. However, note that a pipeline does not have to be a single straight line of transforms applied one after another: think of PCollections as variables and PTransforms as functions applied to these variables: the shape of the pipeline can be an arbitrarily complex processing graph.\n",
    "4. Use IOs to write the final, transformed PCollection(s) to an external source.\n",
    "5. Run the pipeline using the designated Pipeline Runner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.io import fileio\n",
    "import zipfile\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think we can use a beam.map and use a custom function to apply to each element of the beam collection.\n",
    "\n",
    "https://beam.apache.org/documentation/patterns/file-processing/\n",
    "\n",
    "https://beam.apache.org/documentation/transforms/python/elementwise/map/\n",
    "\n",
    "https://stackoverflow.com/questions/51433664/read-files-from-multiple-folders-in-apache-beam-and-map-outputs-to-filenames\n",
    "\n",
    "https://beam.apache.org/get-started/wordcount-example/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_blobs(blob, output_dir):\n",
    "    print(blob_path)\n",
    "    blob_path = blob.metadata.path\n",
    "    blob_name = blob_path.split('/')[1]\n",
    "    check_name = blob_name.split('-')[0]\n",
    "    \n",
    "    temp_dir = download_blob(blob_path)\n",
    "    meta_path = os.path.join(temp_dir.name, \"metadata.json\")\n",
    "    result_path = os.path.join(temp_dir.name, \"result.json\")\n",
    "    extracted_data = {}\n",
    "    \n",
    "    with open(meta_path, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    with open(result_path, \"r\") as f:\n",
    "        result = json.load(f)\n",
    "\n",
    "    target_file = os.path.join(output_dir, blob_name)\n",
    "    if meta['check_name'] != check_name:\n",
    "        meta['check_name'] = check_name\n",
    "\n",
    "        #We need to compress the blob and save it to the target directory\n",
    "        with open('metadata.json', 'w') as outfile:\n",
    "            json.dump(meta, outfile)\n",
    "\n",
    "        with open('result.json', 'w') as outfile:\n",
    "            json.dump(result, outfile)\n",
    "            \n",
    "        with tarfile.open(target_file,\"w:gz\") as tar:\n",
    "            tar.add(os.path.basename('metadata.json'))\n",
    "            tar.add(os.path.basename('result.json'))\n",
    "    else:\n",
    "        copyfile(blob_path, target_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as pipeline:\n",
    "    readable_files = (\n",
    "          pipeline\n",
    "          | fileio.MatchFiles('blobs/*.tar.gz')\n",
    "          | fileio.ReadMatches(compression='auto')\n",
    "          | beam.Map(lambda x: (process_blobs(x, 'clean_blobs_2')))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "En condiciones reales, los archivos residen en un bucket de [google cloud storage](https://cloud.google.com/products/storage/) y hay petabytes de ellos: ¿cómo habrá que modificar la solución a la pregunta 2 para que el source y el sink sean buckets de google cloud en vez de archivos en local?\n",
    "\n",
    "¿Cómo hay que diseñar el código para poder cambiar de source y sink fácilmente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "\n",
    "\n",
    "class GcloudClient():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.storage_client = storage.Client(\"staging-infra-240711\")\n",
    "        # Create a bucket object for our bucket\n",
    "        self.bucket = self.storage_client.get_bucket(\"mapreduce-exercise-manuel\")\n",
    "    \n",
    "    def download_blob_from_bucket(self, blob_name):\n",
    "\n",
    "        # Create a blob object from the filepath\n",
    "        blob = self.bucket.blob(blob_name)\n",
    "        # Download the file to a destination\n",
    "\n",
    "        with NamedTemporaryFile() as f:\n",
    "            blob.download_to_file(f)\n",
    "            f.flush()\n",
    "\n",
    "            temp_dir = TemporaryDirectory()\n",
    "            target_dir = temp_dir.name\n",
    "            with tarfile.open(f.name, \"r:gz\") as tf:\n",
    "                tf.extractall(path=target_dir)\n",
    "\n",
    "            return temp_dir\n",
    "    def upload_blob_to_bucket(self, target_location, source_file_name):\n",
    "        blob = self.bucket.blob(target_location)\n",
    "        blob.upload_from_filename(source_file_name)\n",
    "        \n",
    "\n",
    "def process_blobs_2(blob, output_dir):\n",
    "\n",
    "    blob_path = blob.metadata.path\n",
    "    blob_name = 'blobs/' + blob_path.split('/blobs/')[1]\n",
    "    check_name = blob_name.split('-')[0].split('/')[1]\n",
    "    \n",
    "    gcloud_client = GcloudClient()\n",
    "    \n",
    "    temp_dir = gcloud_client.download_blob_from_bucket(blob_name)\n",
    "    meta_path = os.path.join(temp_dir.name, \"metadata.json\")\n",
    "    result_path = os.path.join(temp_dir.name, \"result.json\")\n",
    "    extracted_data = {}\n",
    "        \n",
    "    with open(meta_path, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    with open(result_path, \"r\") as f:\n",
    "        result = json.load(f)\n",
    "\n",
    "    \n",
    "    source_file_name = os.path.join(output_dir, blob_name)\n",
    "    target_location = os.path.join(\"clean_blobs_3/\", source_file_name)\n",
    "    \n",
    "    if meta['check_name'] != check_name:\n",
    "        meta['check_name'] = check_name\n",
    "\n",
    "        #We need to compress the blob and save it to the target directory\n",
    "        with open('metadata.json', 'w') as outfile:\n",
    "            json.dump(meta, outfile)\n",
    "\n",
    "        with open('result.json', 'w') as outfile:\n",
    "            json.dump(result, outfile)\n",
    "            \n",
    "        with tarfile.open(source_file_name,\"w:gz\") as tar:\n",
    "            tar.add(os.path.basename('metadata.json'))\n",
    "            tar.add(os.path.basename('result.json'))\n",
    "                \n",
    "        gcloud_client.upload_blob_to_bucket(target_location, source_file_name)\n",
    "    else:\n",
    "        gcloud_client.upload_blob_to_bucket(target_location, blob_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_blobs_3/clean_blobs_3/blobs/check_200-2021-09-05T13:43:09.003135.tar.gz\n",
      "clean_blobs_3/clean_blobs_3/blobs/check_200-2021-09-18T05:28:23.003135.tar.gz\n",
      "clean_blobs_3/clean_blobs_3/blobs/check_200-2021-09-29T02:30:43.003135.tar.gz\n",
      "clean_blobs_3/clean_blobs_3/blobs/check_404-2021-08-21T17:26:17.003135.tar.gz\n",
      "clean_blobs_3/clean_blobs_3/blobs/check_404-2021-08-31T12:49:19.003135.tar.gz\n",
      "clean_blobs_3/clean_blobs_3/blobs/check_404-2021-08-31T14:02:25.003135.tar.gz\n",
      "clean_blobs_3/clean_blobs_3/blobs/check_https-2021-08-21T05:58:24.003135.tar.gz\n",
      "clean_blobs_3/clean_blobs_3/blobs/check_login-2021-08-18T15:36:53.003135.tar.gz\n",
      "clean_blobs_3/clean_blobs_3/blobs/check_login-2021-08-24T03:16:43.003135.tar.gz\n",
      "clean_blobs_3/clean_blobs_3/blobs/check_login-2021-09-07T15:40:00.003135.tar.gz\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as pipeline:\n",
    "    readable_files = (\n",
    "          pipeline\n",
    "          | fileio.MatchFiles('gs://mapreduce-exercise-manuel/blobs/*.tar.gz')\n",
    "          | fileio.ReadMatches(compression='auto')\n",
    "          | beam.Map(lambda x: (process_blobs_2(x, 'clean_blobs_3')))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "998e36c689971a6175d8a12518d119b65922ae10d4bf024dd2fd9f854295502d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
